Table of Contents
=================

   * [一、防火墙配置](#一防火墙配置)
   * [二、初始化](#二初始化)
   * [三、初始化集群](#三初始化集群)
      * [1、命令行初始化](#1命令行初始化)
      * [2、通过配置文件进行初始化](#2通过配置文件进行初始化)
      * [3、初始化进行的操作](#3初始化进行的操作)
      * [4、单独部署coredns（选择操作）](#4单独部署coredns选择操作)
      * [5、集群移除节点](#5集群移除节点)
      * [6、kube-proxy开启ipvs](#6kube-proxy开启ipvs)
   * [四、Master操作](#四master操作)
   * [五、Node操作](#五node操作)
   * [六、集群操作](#六集群操作)
   * [七、网络插件部署](#七网络插件部署)
      * [1、master上部署flannel插件](#1master上部署flannel插件)
      * [2、master上部署calico插件](#2master上部署calico插件)
      * [3、性能对比](#3性能对比)
   * [八、安装 Dashboard](#八安装-dashboard)
      * [1、下载yaml文件](#1下载yaml文件)
      * [2、修改配置](#2修改配置)
      * [3、查看dashboard](#3查看dashboard)
      * [4、然后创建一个具有全局所有权限的用户来登录Dashboard：(admin.yaml)](#4然后创建一个具有全局所有权限的用户来登录dashboardadminyaml)
   * [九、问题排查](#九问题排查)
      * [1、coredns异常问题](#1coredns异常问题)
         * [1.1、解决办法](#11解决办法)
      * [2、kubelet异常问题1](#2kubelet异常问题1)
      * [3、kubelet异常问题2](#3kubelet异常问题2)
      
# 一、防火墙配置

```bash
chattr -i /etc/passwd* && chattr -i /etc/group* && chattr -i /etc/shadow* && chattr -i /etc/gshadow*

yum install iptables iptables-services -y

cat > /etc/sysconfig/iptables << \EOF
# Generated by iptables-save v1.4.21 on Thu Aug  1 01:26:09 2019
*filter
:INPUT ACCEPT [0:0]
:FORWARD ACCEPT [0:0]
:OUTPUT ACCEPT [0:0]
:RH-Firewall-1-INPUT - [0:0]
-A INPUT -j RH-Firewall-1-INPUT
-A FORWARD -j RH-Firewall-1-INPUT
-A RH-Firewall-1-INPUT -i lo -j ACCEPT
-A RH-Firewall-1-INPUT -p icmp -m icmp --icmp-type any -j ACCEPT
-A RH-Firewall-1-INPUT -s 192.168.56.0/24 -p tcp -m tcp --dport 22 -j ACCEPT
-A RH-Firewall-1-INPUT -p tcp -m tcp --dport 22 -j DROP
### k8s ###
-A RH-Firewall-1-INPUT -s 192.168.56.11/32 -j ACCEPT
-A RH-Firewall-1-INPUT -s 192.168.56.12/32 -j ACCEPT
-A RH-Firewall-1-INPUT -s 192.168.56.13/32 -j ACCEPT
# serviceSubnet rules
-A RH-Firewall-1-INPUT -s 10.96.0.0/12 -j ACCEPT
# podSubnet rules
-A RH-Firewall-1-INPUT -s 10.244.0.0/16 -j ACCEPT
# keepalived rules
-A RH-Firewall-1-INPUT -p vrrp -j ACCEPT
# port rules
-A RH-Firewall-1-INPUT -s 192.168.56.1/32 -p tcp -m multiport --dports 80,443,1080,6443,16443,30000:32767 -j ACCEPT
### k8s ###
-A RH-Firewall-1-INPUT -m state --state RELATED,ESTABLISHED -j ACCEPT
-A RH-Firewall-1-INPUT -j REJECT --reject-with icmp-host-prohibited
COMMIT
# Completed on Thu Aug  1 01:26:09 2019
EOF

systemctl restart iptables.service
systemctl enable iptables.service

iptables -nvL
```

# 二、初始化

```bash
cat > /etc/hosts << \EOF
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
192.168.56.11 linux-node1 linux-node1.example.com
192.168.56.12 linux-node2 linux-node2.example.com
192.168.56.13 linux-node3 linux-node3.example.com
EOF

systemctl stop firewalld
systemctl disable firewalld

setenforce 0
sed -i 's/SELINUX=.*/SELINUX=disabled/g' /etc/selinux/config
sed -i 's/SELINUXTYPE=.*/SELINUXTYPE=disabled/g' /etc/selinux/config

# 关闭 swap
swapoff -a
#sed -ir 's/.*swap.*/#&/' /etc/fstab
#或
yes | cp /etc/fstab /etc/fstab_bak
cat /etc/fstab_bak |grep -v swap > /etc/fstab

#export Time=`date "+%Y%m%d%H%M%S"`
#cp /etc/fstab /etc/fstab_$Time

cat > /etc/sysctl.d/k8s.conf << \EOF
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
vm.swappiness = 0
EOF

#加载 br_netfilter 模块
modprobe br_netfilter
sysctl -p /etc/sysctl.d/k8s.conf

#创建/etc/sysconfig/modules/ipvs.modules文件,保证在节点重启后能自动加载所需模块
cat > /etc/sysconfig/modules/ipvs.modules <<EOF
#!/bin/bash
modprobe -- ip_vs
modprobe -- ip_vs_rr
modprobe -- ip_vs_wrr
modprobe -- ip_vs_sh
modprobe -- nf_conntrack_ipv4
EOF

chmod 755 /etc/sysconfig/modules/ipvs.modules && bash /etc/sysconfig/modules/ipvs.modules && lsmod | grep -e ip_vs -e nf_conntrack_ipv4

yum install -y ipset ipvsadm

yum install chrony -y
systemctl enable chronyd
systemctl restart chronyd
chronyc sources

yum install -y yum-utils \
  device-mapper-persistent-data \
  lvm2
  
yum-config-manager \
    --add-repo \
    https://download.docker.com/linux/centos/docker-ce.repo
    
#yum list docker-ce --showduplicates | sort -r

yum install -y docker-ce-18.09.9-3.el7.x86_64
systemctl start docker
systemctl enable docker

mkdir -p /data0/docker-data

cat > /etc/docker/daemon.json << \EOF
{
  "exec-opts": ["native.cgroupdriver=systemd"],
  "data-root": "/data0/docker-data",
  "registry-mirrors" : [
    "https://ot2k4d59.mirror.aliyuncs.com/"
  ],
  "insecure-registries": ["reg.hub.com"]
}
EOF

systemctl daemon-reload
systemctl restart docker

cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=0
repo_gpgcheck=0
gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg
        http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF

yum install -y kubelet-1.16.2-0 kubeadm-1.16.2-0 kubectl-1.16.2-0 --disableexcludes=kubernetes

kubeadm version
systemctl daemon-reload
systemctl restart kubelet.service
systemctl enable kubelet.service
systemctl status kubelet

#查看kubelet日志
journalctl -f -u kubelet

#kubelet.service服务位置
ls -l /lib/systemd/system/kubelet.service
```

# 三、初始化集群

## 1、命令行初始化

```bash
#master节点初始化指令
kubeadm init \
  --apiserver-advertise-address=192.168.56.11 \
  --image-repository registry.aliyuncs.com/google_containers \
  --kubernetes-version v1.16.2 \
  --apiserver-bind-port=6443 \
  --service-cidr=10.96.0.0/12 \
  --pod-network-cidr=10.244.0.0/16  #这里使用这个是因为官方flannel使用的这个段地址，不然的话,kube-flannel.yml那里需要调整

#其他节点可以先指定image源，先下载需要的镜像
kubeadm config images pull --image-repository registry.aliyuncs.com/google_containers

#查看集群初始化配置
kubeadm config view

#获取加入集群的指令
kubeadm token create --print-join-command

kubeadm join 192.168.56.11:6443 --token 5avfk1.fwui1smk5utcu7m9     --discovery-token-ca-cert-hash sha256:6730e91a516d8bf3e26d8f5eddd6409a224f8703b94f6ecde2b1fd7481bbbd25

#集群初始化如果遇到问题，可以使用下面的命令进行清理
yes | kubeadm reset
ifconfig cni0 down
ip link delete cni0
ifconfig flannel.1 down
ip link delete flannel.1
rm -rf /var/lib/cni/
rm -f $HOME/.kube/config

systemctl restart kubelet
systemctl status kubelet
journalctl -f -u kubelet
```

## 2、通过配置文件进行初始化

```bash
#在 master 节点配置 kubeadm 初始化文件，可以通过如下命令导出默认的初始化配置：
root># kubeadm config print init-defaults > kubeadm.yaml
```

```bash
#然后根据我们自己的需求修改配置，比如修改 imageRepository 的值，kube-proxy 的模式为 ipvs

如果是 flannel 网络插件的，需要将 networking.podSubnet 设置为默认的 10.244.0.0/16

如果是 Calico 网络插件的，配置成 Calico 的默认网段 podSubnet: 192.168.0.0/16，这个也可以修改Calico的配置文件调整

rm -f kubeadm.yaml

cat > kubeadm.yaml << \EOF
apiVersion: kubeadm.k8s.io/v1beta2
bootstrapTokens:
- groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: abcdef.0123456789abcdef
  ttl: 24h0m0s
  usages:
  - signing
  - authentication
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.56.11 #修改为主节点 IP
  bindPort: 6443
  #controlPlaneEndpoint: 1.1.1.100 #如果前面配置了负载均衡，此处填写vip地址
nodeRegistration:
  criSocket: /var/run/dockershim.sock
  name: linux-node1.example.com
  taints:
  - effect: NoSchedule
    key: node-role.kubernetes.io/master
---
apiServer:
  timeoutForControlPlane: 4m0s
apiVersion: kubeadm.k8s.io/v1beta2
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
controllerManager: {}
dns:
  type: CoreDNS #dns 类型
etcd:
  local:
    dataDir: /var/lib/etcd
#imageRepository: k8s.gcr.io
imageRepository: registry.aliyuncs.com/google_containers #国内不能访问 Google，修改为阿里云
kind: ClusterConfiguration
kubernetesVersion: v1.16.2 # 修改版本号
networking:
  dnsDomain: cluster.local
  # 配置成 flannel 的默认网段
  serviceSubnet: 10.96.0.0/12
  podSubnet: 10.244.0.0/16
scheduler: {}
---
# 开启 IPVS 模式
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
mode: ipvs # kube-proxy 模式
EOF

kubeadm init --config kubeadm.yaml
```

## 3、初始化进行的操作

```bash
初始化操作主要经历了下面15个步骤，每个阶段均输出均使用[步骤名称]作为开头：

    1、[init]：指定版本进行初始化操作
    2、[preflight] ：初始化前的检查和下载所需要的Docker镜像文件。
    3、[kubelet-start] ：生成kubelet的配置文件”/var/lib/kubelet/config.yaml”，没有这个文件kubelet无法启动，所以初始化之前的kubelet实际上启动失败。
    4、[certificates]：生成Kubernetes使用的证书，存放在/etc/kubernetes/pki目录中。
    5、[kubeconfig] ：生成 KubeConfig 文件，存放在/etc/kubernetes目录中，组件之间通信需要使用对应文件。
    6、[control-plane]：使用/etc/kubernetes/manifest目录下的YAML文件，安装 Master 组件。
    7、[etcd]：使用/etc/kubernetes/manifest/etcd.yaml安装Etcd服务。
    8、[wait-control-plane]：等待control-plan部署的Master组件启动。
    9、[apiclient]：检查Master组件服务状态。
    10、[uploadconfig]：更新配置
    11、[kubelet]：使用configMap配置kubelet。
    12、[patchnode]：更新CNI信息到Node上，通过注释的方式记录。
    13、[mark-control-plane]：为当前节点打标签，打了角色Master，和不可调度标签，这样默认就不会使用Master节点来运行Pod。
    14、[bootstrap-token]：生成token记录下来，后边使用kubeadm join往集群中添加节点时会用到
    15、[addons]：安装附加组件CoreDNS和kube-proxy
    
kubectl默认会在执行的用户家目录下面的.kube目录下寻找config文件。这里是将在初始化时[kubeconfig]步骤生成的admin.conf拷贝到.kube/config。
```

## 4、单独部署coredns（选择操作）

```bash
# 不依赖kubeadm的方式，适用于不是使用kubeadm创建的k8s集群，或者kubeadm初始化集群之后，删除了dns相关部署
# 在calico网络中也配置一个coredns # 10.96.0.10 为k8s官方指定的kube-dns地址
rm -f coredns.yaml.sed deploy.sh coredns.yml
wget https://raw.githubusercontent.com/coredns/deployment/master/kubernetes/coredns.yaml.sed
wget https://raw.githubusercontent.com/coredns/deployment/master/kubernetes/deploy.sh
chmod +x deploy.sh
./deploy.sh -i 10.10.0.10 > coredns.yml  #这里从--service-cidr=10.10.0.0/16中选用10.10.0.10作为coredns地址
kubectl apply -f coredns.yml

# 查看
kubectl get pods --namespace kube-system
kubectl get svc --namespace kube-system

#删除coredns
kubectl delete deployment coredns -n kube-system
kubectl delete svc kube-dns -n kube-system
kubectl delete cm coredns -n kube-system
```

## 5、集群移除节点

```bash
1、#移除work节点
在准备移除的 worker 节点上执行
kubeadm reset

2、在第一个 master 节点 demo-master-a-1 上执行
kubectl delete node demo-worker-x-x
#worker 节点的名字可以通过在第一个 master 节点 demo-master-a-1 上执行 kubectl get nodes 命令获得
```

## 6、kube-proxy开启ipvs

```bash
1、#修改ConfigMap的kube-system/kube-proxy中的config.conf，把 mode: "" 改为mode: “ipvs" 保存退出即可

root># kubectl edit cm kube-proxy -n kube-system
configmap/kube-proxy edited

2、#删除之前的proxy pod
root># kubectl get pod -n kube-system |grep kube-proxy |awk '{system("kubectl delete pod "$1" -n kube-system")}'

3、#查看proxy运行状态
root># kubectl get pod -n kube-system | grep kube-proxy

4、#查看日志,如果有 `Using ipvs Proxier.` 说明kube-proxy的ipvs 开启成功!
root># kubectl logs kube-proxy-54qnw -n kube-system
I0518 20:24:09.319160       1 server_others.go:176] Using ipvs Proxier.
W0518 20:24:09.319751       1 proxier.go:386] IPVS scheduler not specified, use rr by default
I0518 20:24:09.320035       1 server.go:562] Version: v1.14.2
I0518 20:24:09.334372       1 conntrack.go:52] Setting nf_conntrack_max to 131072
I0518 20:24:09.334853       1 config.go:102] Starting endpoints config controller
I0518 20:24:09.334916       1 controller_utils.go:1027] Waiting for caches to sync for endpoints config controller
I0518 20:24:09.334945       1 config.go:202] Starting service config controller
I0518 20:24:09.334976       1 controller_utils.go:1027] Waiting for caches to sync for service config controller
I0518 20:24:09.435153       1 controller_utils.go:1034] Caches are synced for service config controller
I0518 20:24:09.435271       1 controller_utils.go:1034] Caches are synced for endpoints config controller
```

# 四、Master操作

```bash
#将 master 节点上面的 $HOME/.kube/config 文件拷贝到 node 节点对应的文件中
mkdir -p $HOME/.kube
yes | cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
chown $(id -u):$(id -g) $HOME/.kube/config

scp $HOME/.kube/config root@linux-node2:$HOME/.kube/config
scp $HOME/.kube/config root@linux-node3:$HOME/.kube/config

#指令补全
yum install bash-completion -y
source <(kubectl completion bash)
echo "source <(kubectl completion bash)" >> ~/.bashrc
```

# 五、Node操作

```bash
#node节点操作
mkdir -p $HOME/.kube
sudo chown $(id -u):$(id -g) $HOME/.kube/config

#加入集群
kubeadm join 192.168.56.11:6443 --token 5avfk1.fwui1smk5utcu7m9     --discovery-token-ca-cert-hash sha256:6730e91a516d8bf3e26d8f5eddd6409a224f8703b94f6ecde2b1fd7481bbbd25
```

# 六、集群操作

```bash
#批量重启docker
docker restart `docker ps -a -q` 

root># kubectl get nodes
NAME                      STATUS     ROLES    AGE     VERSION
linux-node1.example.com   NotReady   master   11m     v1.15.3
linux-node2.example.com   NotReady   <none>   5m9s    v1.15.3
linux-node3.example.com   NotReady   <none>   4m58s   v1.15.3

可以看到是 NotReady 状态，这是因为还没有安装网络插件，接下来安装网络插件，可以在文档 https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/ 中选择我们自己的网络插件，这里我们安装 flannel:

iptables -I RH-Firewall-1-INPUT -s 10.96.0.0/16 -j ACCEPT
service iptables save

root># kubectl get pods -n kube-system
NAME                                              READY   STATUS    RESTARTS   AGE
coredns-5c98db65d4-mk254                          1/1     Running   0          14m
coredns-5c98db65d4-ntz98                          1/1     Running   0          14m
etcd-linux-node1.example.com                      1/1     Running   0          13m
kube-apiserver-linux-node1.example.com            1/1     Running   0          13m
kube-controller-manager-linux-node1.example.com   1/1     Running   0          13m
kube-flannel-ds-amd64-6kx7m                       1/1     Running   0          11m
kube-flannel-ds-amd64-cqfnb                       1/1     Running   0          11m
kube-flannel-ds-amd64-thxx2                       1/1     Running   0          11m
kube-proxy-gdtjg                                  1/1     Running   0          12m
kube-proxy-lcscl                                  1/1     Running   0          14m
kube-proxy-sb7d8                                  1/1     Running   0          12m
kube-scheduler-linux-node1.example.com            1/1     Running   0          13m
kubernetes-dashboard-fcfb4cbc-dqbq9               1/1     Running   0          4m43s

kubectl describe pod/coredns-5c98db65d4-mk254 -n kube-system

#创建Deployment
kubectl run --image=nginx nginx-web-1 --image-pull-policy='IfNotPresent' --replicas=3

#以不同方式暴露出去
kubectl expose deployment nginx-web-1 --port=80 --target-port=80
kubectl expose deployment nginx-web-1 --port=80 --target-port=80 --type=NodePort

root># kubectl exec -it nginx-web-1-5cc49f46bc-kn46r -- \
               sh -c "echo hello>/usr/share/nginx/html/index.html"

root># kubectl get svc -A
default       nginx-web-1   NodePort    10.10.43.53   <none>        80:30163/TCP             101s

root># kubectl get endpoints
nginx-web-1   10.244.154.193:80,10.244.44.193:80,10.244.89.129:80   5m27s

root># curl 10.10.43.53   
hello

#显示iptables规则(注意这里kube-proxy需要使用ipvs模式，上面主机预设的iptables策略才生效)
iptables -nvL --line-number

#删除规则
iptables -D RH-Firewall-1-INPUT 4
```

# 七、网络插件部署

## 1、master上部署flannel插件

```bash
#插件镜像 network: flannel image（因墙的问题，需要从国内源下载）
docker pull quay-mirror.qiniu.com/coreos/flannel:v0.11.0-amd64
docker tag quay-mirror.qiniu.com/coreos/flannel:v0.11.0-amd64  quay.io/coreos/flannel:v0.11.0-amd64

https://www.cnblogs.com/horizonli/p/10855666.html

#部署flannel
rm -f kube-flannel.yml
wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
sed -i 's#image: quay.io/coreos/flannel:v0.11.0-amd64#image: registry.cn-shenzhen.aliyuncs.com/cp_m/flannel:v0.10.0-amd64#g' kube-flannel.yml
kubectl apply -f kube-flannel.yml

#另外需要注意的是如果你的节点有多个网卡的话，需要在 kube-flannel.yml 中使用--iface参数指定集群主机内网网卡的名称，否则可能会出现 dns 无法解析。flanneld 启动参数加上--iface=<iface-name>
args:
- --ip-masq
- --kube-subnet-mgr
- --iface=eth0
```

## 2、master上部署calico插件

```bash
export POD_SUBNET=10.244.0.0/16
rm -f calico.yaml
wget https://docs.projectcalico.org/v3.8/manifests/calico.yaml
sed -i "s#192\.168\.0\.0/16#${POD_SUBNET}#" calico.yaml
kubectl apply -f calico.yaml

https://www.cnblogs.com/goldsunshine/p/10701242.html  k8s网络之Calico网络
```

## 3、性能对比

```bash
https://www.2cto.com/net/201701/591629.html  kubernetes flannel neutron calico三种网络方案性能测试分析
```

# 八、安装 Dashboard

使用 dashboard 最好把浏览器的默认语言设置为英文，不然在进入容器操作的时候会有bug，会出现重影,然后k8s v1.16.x之后，需要使用Dashboard v2.0以上的版本，不然出现在error_outline 未知服务器错误 (404)

## 1、下载yaml文件

```bash
#下载
wget https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-beta5/aio/deploy/recommended.yaml
```

## 2、修改配置
```bash
1、#热更新打补丁的方式修改svc
kubectl apply -f recommended.yaml
kubectl -n kubernetes-dashboard patch svc kubernetes-dashboard -p '{"spec":{"type":"NodePort"}}'
kubectl -n kubernetes-dashboard patch svc kubernetes-dashboard -p '{"spec": {"ports": [{"port":443, "nodePort": 30001}]}}'
kubectl get svc -A|grep kubernetes-dashboard

https://www.jianshu.com/p/f38e1767bf19  使用 kubectl patch 更新 API 对象

2、#手动修改recommended.yaml文件，为了方便访问，修改kubernetes-dashboard的Service定义，指定Service的type类型为NodeType，指定nodePort端口
kubectl delete -f recommended.yaml 

---
kind: Service
apiVersion: v1
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard
  namespace: kubernetes-dashboard
spec:
  type: NodePort  # 新增这一行，指定为NodePort方式
  ports:
    - port: 443
      targetPort: 8443
      nodePort: 30001  # 指定端口为30001
  selector:
    k8s-app: kubernetes-dashboard
---

kubectl apply -f recommended.yaml 

#注：dashboard-metrics-scraper的Service不需要修改

Kubernetes Dashboard 默认部署时，只配置了最低权限的 RBAC

参考文档：https://github.com/kubernetes/dashboard/blob/master/docs/user/access-control/creating-sample-user.md
```

## 3、查看dashboard

```bash
root># kubectl  get pod,deploy,svc -n kubernetes-dashboard
NAME                                             READY   STATUS    RESTARTS   AGE
pod/dashboard-metrics-scraper-76585494d8-ws57d   1/1     Running   0          2m18s
pod/kubernetes-dashboard-6b86b44f87-q26w6        1/1     Running   0          2m18s

NAME                                        READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/dashboard-metrics-scraper   1/1     1            1           2m18s
deployment.apps/kubernetes-dashboard        1/1     1            1           2m18s

NAME                                TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)         AGE
service/dashboard-metrics-scraper   ClusterIP   10.102.114.143   <none>        8000/TCP        2m18s
service/kubernetes-dashboard        NodePort    10.111.191.70    <none>        443:30001/TCP   2m19s

root># curl https://10.111.191.70:443 -k -I
HTTP/1.1 200 OK
Accept-Ranges: bytes
Cache-Control: no-store
Content-Length: 1262
Content-Type: text/html; charset=utf-8
Last-Modified: Mon, 14 Oct 2019 16:39:02 GMT
Date: Wed, 13 Nov 2019 02:25:52 GMT

# 我们可以看到官方的dashboard帮我们启动了web-ui，并且帮我们启动了一个Metric服务
# 但是dashboard默认使用的https的443端口

然后可以通过上面的 https://NodeIP:30001 端口去访问 Dashboard，要记住使用 https，Chrome不生效可以使用Firefox测试：
```

## 4、然后创建一个具有全局所有权限的用户来登录Dashboard：(admin.yaml)

```bash
cat > admin.yaml << \EOF
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: admin
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: "true"
roleRef:
  kind: ClusterRole
  name: cluster-admin
  apiGroup: rbac.authorization.k8s.io
subjects:
- kind: ServiceAccount
  name: admin
  namespace: kube-system

---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: admin
  namespace: kube-system
  labels:
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
EOF

kubectl apply -f admin.yaml

kubectl delete -f admin.yaml

#获取token
kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin | awk '{print $1}')
```

https://192.168.56.12:31513

然后用上面的base64解码后的字符串作为token登录Dashboard即可： k8s dashboard

最终我们就完成了使用 kubeadm 搭建 v1.15.3 版本的 kubernetes 集群、coredns、ipvs、flannel。 

# 九、问题排查

## 1、coredns异常问题

  ![coredns异常问题](https://github.com/Lancger/opsfull/blob/master/images/coredns-01.png)

```
E1006 12:30:53.935744       1 reflector.go:134] github.com/coredns/coredns/plugin/kubernetes/controller.go:317: Failed to list *v1.Endpoints: Get https://10.10.0.1:443/api/v1/endpoints?limit=500&resourceVersion=0: dial tcp 10.10.0.1:443: connect: no route to host
E1006 12:30:53.935744       1 reflector.go:134] github.com/coredns/coredns/plugin/kubernetes/controller.go:317: Failed to list *v1.Endpoints: Get https://10.10.0.1:443/api/v1/endpoints?limit=500&resourceVersion=0: dial tcp 10.10.0.1:443: connect: no route to host
log: exiting because of error: log: cannot create log: open /tmp/coredns.coredns-bccdc95cf-vlqxk.unknownuser.log.ERROR.20191006-123053.1: no such file or directory
```

### 1.1、解决办法

```
实际上是主机防火墙的问题，需要添加
iptables -A RH-Firewall-1-INPUT -s 10.10.0.0/16 -j ACCEPT

其他参考
https://medium.com/@cminion/quicknote-kubernetes-networking-issues-78f1e0d06e12
https://github.com/coredns/coredns/issues/2325  
```

## 2、kubelet异常问题1

```
问题现象：

kubelet fails to get cgroup stats for docker and kubelet services

解决办法:

cat > /etc/sysconfig/kubelet <<\EOF
KUBELET_EXTRA_ARGS=--runtime-cgroups=/systemd/system.slice --kubelet-cgroups=/systemd/system.slice
EOF

systemctl daemon-reload
systemctl restart kubelet
systemctl status kubelet

#查看kubelet日志
journalctl -f -u kubelet

https://stackoverflow.com/questions/46726216/kubelet-fails-to-get-cgroup-stats-for-docker-and-kubelet-services  

https://www.twblogs.net/a/5cc87d63bd9eee1ac2ed736b
```

## 3、kubelet异常问题2

```
failed to create kubelet: misconfiguration: kubelet cgroup driver: "cgroupfs" is different from docker cgroup driver: "systemd"

#解决办法
添加如下内容--cgroup-driver=systemd

[root@tw19336 ~]# cat /usr/lib/systemd/system/kubelet.service.d/10-kubeadm.conf
# Note: This dropin only works with kubeadm and kubelet v1.11+
[Service]
Environment="KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --cgroup-driver=systemd"
Environment="KUBELET_CONFIG_ARGS=--config=/var/lib/kubelet/config.yaml"
# This is a file that "kubeadm init" and "kubeadm join" generates at runtime, populating the KUBELET_KUBEADM_ARGS variable dynamically
EnvironmentFile=-/var/lib/kubelet/kubeadm-flags.env
# This is a file that the user can use for overrides of the kubelet args as a last resort. Preferably, the user should use
# the .NodeRegistration.KubeletExtraArgs object in the configuration files instead. KUBELET_EXTRA_ARGS should be sourced from this file.
EnvironmentFile=-/etc/sysconfig/kubelet
ExecStart=
ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS


systemctl daemon-reload
systemctl restart kubelet
systemctl status kubelet
https://www.cnblogs.com/hongdada/p/9771857.html
```

参考文档：

https://www.cnblogs.com/liyongjian5179/p/11417794.html   使用kubeadm安装Kubernetes 1.15.3 并开启 ipvs

https://www.jianshu.com/p/8bc61078bded 

https://www.cnblogs.com/lovesKey/p/10888006.html  centos7下用kubeadm安装k8s集群并使用ipvs做高可用方案

https://github.com/kubernetes/dashboard/wiki/Creating-sample-user

https://www.qikqiak.com/post/use-kubeadm-install-kubernetes-1.15.3/ 

https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/  官方文档 

https://www.jianshu.com/p/d0933d6ae162 kubeadm 1.15 安装

https://yq.aliyun.com/articles/680080/  单独部署coredns

https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/ha-topology/#stacked-etcd-topology  etcd-stacked-cluster

https://www.kubernetes.org.cn/5021.html  etcd 集群运维实践
